\documentclass[12pt,journal,compsoc]{IEEEtran}

\newcommand{\subparagraph}{}

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{1ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0.5ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

\usepackage{graphicx}
\graphicspath{ {../src/img/} }
 
\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Assignment 3}

\author{Shubham~Singla,
        Rupesh Kashyap,
        and~Harsh~Kumar}% <-this % stops a space


% make the title area
\maketitle

\section{Introduction}
\IEEEPARstart{F}{ollowing} report contains observation for the $3$ data sets namely, MNIST, Signal data and Amazon Helpfulness ratio dataset. The observations contain the performance of different machine learning algorithms on the above mentioned data sets. Different model architecture were tried on these datasets to achieve good results which have been summarized below in the form of graphs, accuracy, precision, recall, f-score.

\section{MNIST DataSet}
\noindent Each of 'Healthy', 'Surgery' and 'Medication' are assigned a number $0$, $1$ and $2$ respectively. Thus, it reduces to a problem of having labeled data with $3$ classes and $3$ features for each data point. The data was observed in 3D and 2D using PCA. Standardization of data followed by a 75/25 split into train and dev set was done. KNN, Bayes and Naive-Bayes Classifier with different class conditional densities as GMM, Multivariate Normal and Parzen Window for Supervised Learning and K-Means clustering for Unsupervised Learning were applied.
\begin{center}
\includegraphics[scale=0.35]{2d_medical.png}

{\small Fig. 1 Visualization of Data in 2D}
\end{center}

\subsection{Observations}
\noindent In all the below tables, F(C$i$) denotes the F-Score for class $i$. Comp. denotes the number of components used in Gaussian Mixture Models (GMM). Vol. denotes the volume parzen window with HC being Hypercube and G being Gaussian kernels. Cf-Type is the type of classifier where B(GMM) denotes Bayes Classifier with class conditional density as GMM, NB(Pz) denotes Naive Bayes classifier with class conditional density as Parzen Window Estimation and MN denotes multinormal. Param. denotes the parameters of the corresponding classifier and class conditional density. Acc. denotes the accuracy.

\noindent In unsupervised learning results, Ngh. denotes the neighbours and M.P. denotes the Minkowski parameter of KNN. In the K-Means result, Iter. denotes the iteration used in K-Means.
\vspace*{0.2 cm}
\subsubsection{Supervised Learning Algorithms}
\vspace*{0.2 cm}

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{Bayes Classifier - GMM (Dev Set)}\\
\hline
Comp. & Accuracy & F(C1) & F(C2) & F(C3)\\
\hline
$2$ &$0.916$ &$0.9424$ &$0.9171$ &$0.8875$\\
\hline
$3$ &$0.9187$ &$0.9424$ &$0.9199$ &$0.8931$\\
\hline
$4$ &$0.9187$ &$9463$ &$.9174$ &$0.8917$\\
\hline
$5$ &$0.9187$ &$0.9465$ &$0.9165$ &$0.8926$\\
\hline
\end{tabular}
\\
\vspace*{0.2cm}

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{Bayes - Parzen Window (Dev Set)}\\
\hline
Vol & Acc. & F(C1) & F(C2) & F(C3)\\
\hline
$HC(0.5)$ &$0.9013$ &$0.9196$ &$9068$ &$8753$\\
\hline
$HC(1)$ &$0.884$ &$0.913$ &$0.8919$ &$0.8446$\\
\hline
$G(0.5)$ &$0.8787$ &$0.9107$ &$0.8855$ &$0.8377$\\
\hline
$G(1)$ &$0.844$ &$0.9087$ &$0.833$ &$0.7897$\\
\hline
\end{tabular}
\\
\vspace*{0.2 cm}

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{Naive Bayes Classifier - GMM (Dev Set)}\\
\hline
Comp. & Accuracy & F(C1) & F(C2) & F(C3)\\
\hline
$2$ &$0.9213$ &$0.9491$ &$0.9225$ &$0.8908$\\
\hline
$3$ &$0.9213$ &$0.9474$ &$0.9225$ &$0.8922$\\
\hline
$4$ &$0.92$ &$0.9455$ &$0.9245$ &$0.8874$\\
\hline
$5$ &$0.913$ &$0.9379$ &$9145$ &$0.8855$\\
\hline
\end{tabular}

\vspace*{0.2 cm}

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{N. Bayes - Parzen Window (Dev Set)}\\
\hline
Vol & Acc. & F(C1) & F(C2) & F(C3)\\
\hline
$HC(0.5)$ &$0.9$ &$0.9311$ &$0.9009$ &$0.867$\\
\hline
$HC(1)$ &$0.8853$ &$0.927$ &$0.8845$ &$0.845$\\
\hline
$G(0.5)$ &$0.872$ &$0.9195$ &$0.875$ &$0.8205$\\
\hline
$G(1)$ &$0.792$ &$0.8512$ &$0.7469$ &$0.7721$\\
\hline
\end{tabular}

\vspace*{0.2 cm}
\begin{table*}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\multicolumn{6}{c}{Classifier with Best Parameters on Medical Test Data}\\
\hline
Cf-Type & Para. & Acc. & F(C1) & F(C2) & F(C3)\\
\hline
B(GMM) &$4$ &$0.9$ &$0.9276$ &$0.8824$ &$0.8916$\\
\hline
B(Pz) &G(0.25) &$0.888$ &$0.9159$ &$0.8784$ &$0.8722$\\
\hline
B(MN) &- &$0.5027$ &$6251$ &$0.4137$ &$0.3976$\\
\hline
NB(GMM) &$3$ &$0.899$ &$0.9277$ &$0.8822$ &$0.8884$ \\
\hline
NB(Pz) &$HC(0.5)$ &$0.8927$ &$0.9216$ &$0.8801$ &$0.878$\\
\hline
\end{tabular}
\end{table*}

\vspace*{0.2 cm}
\begin{tabular}{|c|c|c|c|c|c|}
\multicolumn{6}{c}{KNN on 3D Data Set (Dev Set)}\\
\hline
Ngh. & M.P. & Acc. & F(C1) & F(C2) & F(C3)\\
\hline
$1$ &$2$ &$0.8733$ &$0.9047$ &$0.8636$ &$0.8517$\\
\hline
$5$ &$2$ &$0.904$ &$0.9246$ &$0.9$ &$0.887$\\
\hline
$10$ &$2$ &$0.9107$ &$0.9284$ &$0.9091$ &$0.8937$\\
\hline
$15$ &$2$ &$0.916$ &$0.9286$ &$0.922$ &$0.8964$\\
\hline
$1$ &$inf$ &$0.872$ &$0.9087$ &$0.8582$ &$0.8493$\\
\hline
$5$ &$inf$ &$0.8933$ &$0.9187$ &$0.8831$ &$0.8785$\\
\hline
$10$ &$inf$ &$0.908$ &$0.9218$ &$0.9091$ &$0.8922$\\
\hline
$15$ &$inf$ &$0.9133$ &$0.926$ &$0.917$ &$0.8964$\\
\hline
$1$ &$inf$ &$0.6333$ &$0.6613$ &$0.6463$ &$0.59$\\
\hline
$5$ &$inf$ &$0.7627$ &$0.7992$ &$0.7416$ &$0.7441$\\
\hline
$10$ &$-inf$ &$0.82$ &$0.8523$ &$0.8031$ &$0.8017$\\
\hline
$15$ &$inf$ &$0.8373$ &$0.8598$ &$0.8111$ &$0.8398$\\
\hline
\end{tabular}


\end{center}
KNN on $3$ dimensional data with $5$ neighbors and Euclidean distance performed best.\\
Performance on test data1-\\ 
Accuracy - $0.8843$\\ F-Score (Class 1) - $0.9169$\\
F-Score (Class 2) - $0.8654$\\ F-Score (Class 3) - $0.8725$\\ Macro Average F-Score - $0.8858$\\
Micro Average F-Score - $0.8843$\\
KNN was run on 2D dataset obtained after PCA, but the performance decreased. Hence, further analysis was not done on 2D data.
\subsubsection{Unsupervised Learning Algorithm}
\vspace*{0.1 cm}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{K-Means (Test Set)}\\
\hline
Iter. & Accuracy & F(C1) & F(C2) & F(C3)\\
\hline
$40$ &$0.57$ &$0.4593$ &$0.5835$ &$0.4214$\\
\hline
$80$ &$0.65$ &$0.6593$ &$0.5312$ &$0.5871$\\
\hline
\end{tabular}
\end{center}
\subsection{Conclusion}
\noindent Maximum accuracy od $0.9$ for Bayes classifier is obtained using a Gaussian Mixture model with 4 components. For Parzen window density estimation accuracy increases as the size of window decreases. Very small values were not chosen to avoid empty volumes. Assuming Gaussian window with cube length of $0.25$, accuracy of 0.888 was obtained. It increases slightly in case of Naive Bayes to $0.8927$ using a hypercube with cube length of 0.5. For Multivaraite Normal class conditional distribution, bayes classifier gives an accuracy of $0.5027$. KNN also gives a high accuracy of 0.8843 considering 5 neighbours using Euclidean distance. It was also observed that on applying PCA and reducing the data to 2 dimensions resulted in decrease in the accuracy. 
\section{Signal Data}
\noindent The labels corresponding to each data point were in binary format denoting BOARDED AND NON-BOARDED CASES. The discrete features in the Railway data included the sex column and Prefered class column. Both of them were encoded first with female as $1$ and male as $0$ in the sex column and $FIRST_AC$ as $1$, $SECOND_AC$ as $2$, $THIRD_AC$ as $3$ and $NO_PREF$ as $0$. Different Machine Learning models were tested first on this encoding. Later the sex and prefered class feature columns were encoded using one-hot encoding with the prefered column converted to three distinct columns for first, second and third class leaving the no pref case for statistical independence. 

\begin{enumerate}
\item Using PCA to visualize the data in 2 dimensions.

\item Splitting the dataset using the data utils helper module to divide the entire dataset into three parts. Train: Test: Validation=0.8:0.1:0.1

\end{enumerate}
\pagebreak
\subsection{Observations}
\subsubsection{Supervised Learning Algorithms}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{Bayes Classifier - GMM (Railway Dev Set)}\\
\hline
Comp. & Accuracy & F(C1) & F(C2) & Micro F\\
\hline
$2$ &$0.716$ &$0.5024$ &$0.8071$ &$0.7175$ \\
\hline
$3$ &$0.7387$ &$0.5724$ &$0.8099$ &$0.7331$ \\
\hline
$4$ &$0.7387$ &$0.5763$ &$.8074$ &$0.7317$ \\
\hline
$5$ &$0.7387$ &$0.5763$ &$.8074$ &$0.7317$ \\
\hline
\end{tabular}
\newline

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{Best Classifier on Railway Test Data}\\
\hline
Cf-Type & Para. & Acc. & F(C1) & F(C2) \\
\hline
B(GMM) &$3$ &$0.79$ &$0.6976$ &$0.8124$ \\
\hline
B(MN) &- &$0.698$ &$0.5759$ &$0.7684$ \\
\hline
B(PCA) &- &$0.5727$ &$0.3462$ &$0.7237$ \\
\hline
NB(GMM) &$1$ &$0.649$ &$0.6477$ &$0.7822$  \\
\hline
NB(mix) &$-$ &$0.7127$ &$0.6216$ &$0.7601$ \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{KNN on one-hot Data Set (Dev Set)}\\
\hline
Ngh. & M.P. & Acc. & F(C1) & F(C2) \\
\hline
$1$ &$1$ &$0.7033$ &$0.6047$ &$0.7636$ \\
\hline
$5$ &$1$ &$0.724$ &$0.6246$ &$0.78$\\
\hline
$10$ &$1$ &$0.7307$ &$0.6484$ &$0.7891$ \\
\hline
$15$ &$1$ &$0.746$ &$0.6486$ &$0.792$ \\
\hline
$1$ &$2$ &$0.692$ &$0.5987$ &$0.7582$ \\
\hline
$5$ &$2$ &$0.7133$ &$0.6087$ &$0.7831$ \\
\hline
$10$ &$2$ &$0.728$ &$0.6418$ &$0.7791$ \\
\hline
$15$ &$2$ &$0.7333$ &$0.616$ &$0.7917$ \\
\hline
$1$ &$3$ &$0.7433$ &$0.6113$ &$0.8063$ \\
\hline
$5$ &$3$ &$0.7227$ &$0.6092$ &$0.7816$ \\
\hline
$10$ &$3$ &$0.72$ &$0.6323$ &$0.6831$ \\
\hline
$15$ &$3$ &$0.7373$ &$0.6298$ &$0.7911$ \\
\hline
\end{tabular}
\end{center}

\noindent Using Bayes Classifier after reducing dimesnions such that percent variance retained is 99 resulted in drop of 20 percent in accuracy. 

\subsubsection{Unsupervised Learning Algorithms}

\noindent K-means Clustering gives poor predictions as the euclidean distance kernel is not a good measure. Kmeans Accuracy on Dev Set =  49.5419

\pagebreak
\subsection{Conclusion}
\noindent Bayes classifier with gaussian mixture model of 3 components gives best accuracy of  $79$ percent on test data. KNN classifier gives a reasonable performance with about $77$ percent test accuracy with $15$ neighbours and Minkowski Parameter = 1. The K Means Clustering algorithms using euclidean distance as kernel fails badly which suggest that eucleadean distance does not hold any good similarity between data points. One thing that is confusing is that though the features fairly seem independent of each other, the naive bayes failed to improve the accuracy significantly. One of reasons maybe poor MLE estimate of multinomial distribution and also the hyperparameters in bayesian estimation for the prior districution effect the results greatly. Another interesting phenomenon observed in this dataset is that even though PCA captures 99 percent of the variance explained by the data, the training on transformed data causes the accuracy of mixture modeled bayes classifier to drop drastically by almost 20 percent. This may be because of the under lying non linear relationships between the feature spaces which are not captured by PCA. Overall the bayesian decision models doesn’t appear to work very significantly on discrete categorical feature spaces.
\section{Amazon Review Helpfulness}
\noindent We used the review dataset of healthcare products provided by Mcauley et al. [1]. ​The dataset includes reviews with their corresponding ratings, text, helpfulness votes and product metadata. Amazon has a voting system whereby community members provide helpful votes to rate
the reviews of other community members. Helpfulness is available in the form of ratio of number of people who found the review helpful to the total number of people who voted.
We learnt a neural network architecture which labels the review as class 1 having helpfulness $> 0.5.$ and class 0 otherwise. Two main features used are readability index and subjectivity of the review.
\begin{center}
\includegraphics[scale=0.7]{Amazon_arch.PNG}

{\small Fig 1.  Neural Network Architecture}
\end{center}
\subsection{Observations}
\subsubsection{Data Distribution}
\begin{center}
\includegraphics[scale=0.6]{Amazon_rating.PNG}

{\small Fig 2. Ratings Histogram}
\end{center}

\begin{center}
\includegraphics[scale=0.6]{Amazon_helpfulness.PNG}

{\small Fig 3. Helpfulness Ratio Histogram}
\end{center}

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{3}{c}{Random Forest Model}\\
\hline
Class & Precision & Recall & F-Score\\
\hline
$1$ &$0.52$ &$0.48$ &$0.5$\\
\hline
$0$ &$0.45$ &$0.36$ &$0.4$\\
\hline
\end{tabular}
\vspace{0.2cm}

\begin{tabular}{|c|c|c|c|c|}
\multicolumn{3}{c}{Neural Network Model}\\
\hline
Class & Precision & Recall & F-Score\\
\hline
$1$ &$0.69$ &$0.65$ &$0.67$\\
\hline
$0$ &$0.587$ &$0.632$ &$0.608$\\
\hline
\end{tabular}
\vspace{0.2cm}

\begin{center}
\includegraphics[scale=0.4]{Amazon_loss.PNG}

{\small Fig 4. Neural Network Model Loss}
\end{center}

\begin{center}
\includegraphics[scale=0.4]{Amazon_accuracy.PNG}

{\small Fig 5. Neural Network Model Accuracy}
\end{center}

\subsection{Conclusion}
\noindent Maximum F-score of $0.67$ for class 1 is obtained using a complex neural network model architecture as described. The results are not good since we observed that the total number of unique words present were 80474 out of which 42552 were not present in the Word2Vec model due to spelling errors. These are assigned a zero vector in our model. This can be increased by either using a spell corrector or using Fasttext word embeddings provided by Facebook Research.

\subsection{References}

\noindent [1] Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering, R. He, J. McAuley, ​WWW​, 2016\\
\noindent [2] Estimating the Helpfulness and Economic Impact of Product Reviews: Mining Text and Reviewer Characteristics Anindya Ghose and Panagiotis G. Ipeirotis, IEEE, 2011\\
\noindent [3] Add the mnist train set website\\
\noindent [3] Add any other reference\\
\end{document}


